{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWZo3S7FVI3q5f9fjClFOe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhmungale/DataScience_Assignements/blob/main/GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.1Implement a simple GAN architecture to generate random images (like noise or basic shapes) using\n",
        "TensorFlow/KerasB"
      ],
      "metadata": {
        "id": "4t3JZ0KRglu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, noise_dim))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    fig, axs = plt.subplots(dim[0], dim[1], figsize=figsize)\n",
        "\n",
        "    # If only one row, axs is 1D; otherwise, it's 2D.\n",
        "    count = 0\n",
        "    for i in range(dim[0]):\n",
        "        for j in range(dim[1]):\n",
        "            # If axs is a 2D array, axs[i, j] will work. Otherwise, axs is a 1D array\n",
        "            if dim[0] == 1:  # if we only have one row of images\n",
        "                axs[j].imshow(generated_images[count, :, :, 0], cmap='gray')\n",
        "                axs[j].axis('off')\n",
        "            else:\n",
        "                axs[i, j].imshow(generated_images[count, :, :, 0], cmap='gray')\n",
        "                axs[i, j].axis('off')\n",
        "            count += 1\n",
        "\n",
        "    plt.savefig(f\"gan_generated_image_{epoch}.png\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "r9WmqEblgvKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.2.Implement the discriminator for a GAN with an image input of shape (28, 28)B"
      ],
      "metadata": {
        "id": "9ZOpT87Jg0Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the shape of the image input (28x28 grayscale images)\n",
        "img_shape = (28, 28, 1)\n",
        "\n",
        "# Build the Discriminator Model\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=img_shape))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))  # LeakyReLU activation\n",
        "    model.add(layers.Dropout(0.25))  # Dropout for regularization\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Third convolutional layer\n",
        "    model.add(layers.Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully connected layer with sigmoid activation\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Output a value between 0 and 1 (real/fake)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the Discriminator\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Compile the Discriminator Model\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',  # Binary crossentropy for classification\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),  # Adam optimizer\n",
        "    metrics=['accuracy']  # Track accuracy during training\n",
        ")\n",
        "\n",
        "# Print the Discriminator Model summary to check the architecture\n",
        "discriminator.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "CJOp-o_Qg4u_",
        "outputId": "c74768f8-aacc-40cd-85dd-7ad9cfc0f359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_28 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_29 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m295,168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_30 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │           \u001b[38;5;34m4,097\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,097</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m373,761\u001b[0m (1.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">373,761</span> (1.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m373,761\u001b[0m (1.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">373,761</span> (1.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUe.3.Train the generator to produce simple digits (using noise as input) and plot the generated imagesB"
      ],
      "metadata": {
        "id": "-2vUAGARhAvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Generator (updated)\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Fully connected layer\n",
        "    model.add(layers.Dense(7 * 7 * 128, input_dim=noise_dim))  # 7*7*128 = 3136\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "\n",
        "    # Reshape to 7x7x128\n",
        "    model.add(layers.Reshape((7, 7, 128)))\n",
        "\n",
        "    # Upsampling layers\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))  # 14x14x128\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding='same'))  # 28x28x64\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, kernel_size=3, strides=1, padding='same', activation='tanh'))  # 28x28x1\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vDwG83TzhB8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUe.4. Implement WGAN by modifying the loss function in the GANB"
      ],
      "metadata": {
        "id": "77sjBV72jQ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the dimensions of the random noise vector\n",
        "noise_dim = 100\n",
        "\n",
        "# Define the shape of the generated images (e.g., 28x28 for MNIST-like images)\n",
        "img_shape = (28, 28, 1)\n",
        "\n",
        "# Build the Generator Model\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(256, input_dim=noise_dim))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Dense(1024))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(layers.Reshape(img_shape))\n",
        "    return model\n",
        "\n",
        "# Build the Critic (Discriminator) Model\n",
        "def build_critic():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=img_shape))\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dense(1))  # No activation function for the critic\n",
        "    return model\n",
        "\n",
        "# Compile the Critic Model\n",
        "critic = build_critic()\n",
        "critic.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005))\n",
        "\n",
        "# Build the WGAN (stacked generator and critic)\n",
        "def build_wgan(generator, critic):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(generator)\n",
        "    critic.trainable = False\n",
        "    model.add(critic)\n",
        "    return model\n",
        "\n",
        "# Create instances of generator and WGAN models\n",
        "generator = build_generator()\n",
        "wgan = build_wgan(generator, critic)\n",
        "\n",
        "# Compile the WGAN model\n",
        "wgan.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005))\n",
        "\n",
        "# Function to train the WGAN\n",
        "def train_wgan(epochs, batch_size, sample_interval=50):\n",
        "    # Load the MNIST dataset (we'll use this as our \"real\" dataset)\n",
        "    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    X_train = X_train / 127.5 - 1.0  # Rescale to [-1, 1]\n",
        "    X_train = np.expand_dims(X_train, axis=3)  # Add channel dimension\n",
        "\n",
        "    # Labels for real and fake images (not used in WGAN)\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train the Critic\n",
        "\n",
        "        # Train the critic 5 times per generator update\n",
        "        for _ in range(5):  # Number of critic updates\n",
        "            # Select a random batch of real images from the dataset\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_images = X_train[idx]\n",
        "\n",
        "            # Generate fake images using the generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "            fake_images = generator.predict(noise)\n",
        "\n",
        "            # Train the critic on real and fake images\n",
        "            c_loss_real = critic.train_on_batch(real_images, real_labels)\n",
        "            c_loss_fake = critic.train_on_batch(fake_images, fake_labels)\n",
        "            c_loss = 0.5 * np.add(c_loss_real, c_loss_fake)\n",
        "\n",
        "            # Clip the critic's weights\n",
        "            for layer in critic.layers:\n",
        "                if hasattr(layer, 'kernel'):\n",
        "                    layer.kernel.assign(tf.clip_by_value(layer.kernel, -0.01, 0.01))\n",
        "\n",
        "        # Train the Generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "        g_loss = wgan.train_on_batch(noise, real_labels)  # Generator tries to fool the critic\n",
        "\n",
        "        # Print the progress\n",
        "        if epoch % sample_interval == 0:\n",
        "            print(f\"{epoch} [C loss: {c_loss}] [G loss: {g_loss}]\")\n",
        "            sample_images(epoch)\n",
        "\n",
        "# Function to save generated images\n",
        "def sample_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, noise_dim))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    # Check the shape of generated_images before plotting\n",
        "    print(generated_images.shape)  # Debugging print to check shape\n",
        "\n",
        "    fig, axs = plt.subplots(dim[0], dim[1], figsize=figsize)\n",
        "    count = 0\n",
        "\n",
        "    # If there's only one row of images, axs will be a 1D array, not 2D.\n",
        "    # So, we need to ensure axs is always 2D by checking its shape.\n",
        "    if dim[0] == 1:\n",
        "        axs = np.expand_dims(axs, axis=0)\n",
        "\n",
        "    for i in range(dim[0]):\n",
        "        for j in range(dim[1]):\n",
        "            # Make sure to handle the correct number of images\n",
        "            axs[i, j].imshow(generated_images[count, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            count += 1\n",
        "    plt.savefig(f\"wgan_generated_image_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "p8aTgpYyjWTU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.5.\u0019B Use a trained generator to generate a batch of fake images and display them"
      ],
      "metadata": {
        "id": "YhNLgNAjlVvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save generated images\n",
        "def sample_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, noise_dim))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    # Check the shape of generated_images before plotting\n",
        "    print(generated_images.shape)  # Debugging print to check shape\n",
        "\n",
        "    fig, axs = plt.subplots(dim[0], dim[1], figsize=figsize)\n",
        "    count = 0\n",
        "    for i in range(dim[0]):\n",
        "        for j in range(dim[1]):\n",
        "            axs[i, j].imshow(generated_images[count, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            count += 1\n",
        "    plt.savefig(f\"wgan_generated_image_{epoch}.png\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "0eR4ZBrclgAm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.6.Create a StyleGAN-inspired architecture that outputs high-resolution imagesB"
      ],
      "metadata": {
        "id": "i5ibTnqtnPrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Latent vector size (input to the network)\n",
        "latent_dim = 512\n",
        "# Image shape (output resolution)\n",
        "img_shape = (1024, 1024, 3)\n",
        "\n",
        "# Define the Mapping Network\n",
        "def build_mapping_network():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(latent_dim,)))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))  # Style vector w\n",
        "    return model\n",
        "\n",
        "# Define the StyleGAN Generator Network\n",
        "def build_generator():\n",
        "    # Mapping network for style vector\n",
        "    style_input = layers.Input(shape=(latent_dim,))\n",
        "    style_network = build_mapping_network()\n",
        "    w = style_network(style_input)\n",
        "\n",
        "    # Synthesis network\n",
        "    x = layers.Dense(4*4*512)(w)\n",
        "    x = layers.Reshape((4, 4, 512))(x)\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 8x8\n",
        "    x = layers.Conv2D(512, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Progressively upscale\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 16x16\n",
        "    x = layers.Conv2D(256, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 32x32\n",
        "    x = layers.Conv2D(128, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 64x64\n",
        "    x = layers.Conv2D(64, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 128x128\n",
        "    x = layers.Conv2D(32, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 256x256\n",
        "    x = layers.Conv2D(16, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 512x512\n",
        "    x = layers.Conv2D(8, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 1024x1024\n",
        "    x = layers.Conv2D(4, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Final output layer\n",
        "    img = layers.Conv2D(3, kernel_size=3, padding='same', activation='tanh')(x)\n",
        "\n",
        "    # Create and return the model\n",
        "    model = tf.keras.Model(inputs=style_input, outputs=img)\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator\n",
        "def build_discriminator():\n",
        "    input_img = layers.Input(shape=img_shape)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=2, padding='same')(input_img)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)  # Binary output (real or fake)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_img, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Instantiate the models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
        "\n",
        "# Build the GAN model (combine generator and discriminator)\n",
        "discriminator.trainable = False  # During GAN training, freeze the discriminator\n",
        "\n",
        "z = layers.Input(shape=(latent_dim,))\n",
        "gen_img = generator(z)\n",
        "validity = discriminator(gen_img)\n",
        "\n",
        "wgan = tf.keras.Model(inputs=z, outputs=validity)\n",
        "wgan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "\n",
        "# Function to save generated images\n",
        "def save_generated_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, latent_dim))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    fig, axs = plt.subplots(dim[0], dim[1], figsize=figsize)\n",
        "    count = 0\n",
        "    for i in range(dim[0]):\n",
        "        for j in range(dim[1]):\n",
        "            axs[i, j].imshow(generated_images[count])\n",
        "            axs[i, j].axis('off')\n",
        "            count += 1\n",
        "    plt.savefig(f\"generated_image_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Function to train the GAN model\n",
        "def train_gan(epochs, batch_size, sample_interval=50):\n",
        "    # Load dataset (e.g., CIFAR-10, here we use random data for illustration)\n",
        "   # (X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "    #X_train = X_train / 127.5 - 1.0  # Rescale to [-1, 1]\n",
        "    #X_train = np.expand_dims(X_train, axis=3)  # Add channel dimension\n",
        "    from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "\n",
        "    # Resize CIFAR-10 images to 1024x1024\n",
        "    # Correct the indentation of this block\n",
        "    (X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data() # load the cifar10 data\n",
        "    X_train_resized = np.array([np.array(array_to_img(img)).resize((1024, 1024)) for img in X_train])\n",
        "    X_train_resized = X_train_resized / 127.5 - 1.0  # Rescale to [-1, 1]\n",
        "\n",
        "\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train Discriminator\n",
        "\n",
        "        # Select a random batch of real images\n",
        "        idx = np.random.randint(0, X_train_resized.shape[0], batch_size) # use X_train_resized instead of X_train\n",
        "        real_images = X_train_resized[idx]\n",
        "\n",
        "        # Generate fake images\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator (via the GAN model)\n",
        "        g_loss = wgan.train_on_batch(noise, real_labels)\n",
        "\n",
        "        # Print the progress\n",
        "        if epoch % sample_interval == 0:\n",
        "            print(f\"{epoch} [D loss: {d_loss}] [G loss: {g_loss}]\")\n",
        "            save_generated_images(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWmwzeL6nXC0",
        "outputId": "6a1ed8f4-9009-4ab7-8a5e-e0706b0eeb79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.7.Implement the Wasserstein loss function for GAN trainingB"
      ],
      "metadata": {
        "id": "4Qc6sZpYn68W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Latent vector size (input to the network)\n",
        "latent_dim = 512\n",
        "# Image shape (output resolution)\n",
        "img_shape = (1024, 1024, 3)\n",
        "\n",
        "# Define the Mapping Network\n",
        "def build_mapping_network():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(latent_dim,)))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))  # Style vector w\n",
        "    return model\n",
        "\n",
        "# Define the StyleGAN Generator Network\n",
        "def build_generator():\n",
        "    # Mapping network for style vector\n",
        "    style_input = layers.Input(shape=(latent_dim,))\n",
        "    style_network = build_mapping_network()\n",
        "    w = style_network(style_input)\n",
        "\n",
        "    # Synthesis network\n",
        "    x = layers.Dense(4*4*512)(w)\n",
        "    x = layers.Reshape((4, 4, 512))(x)\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 8x8\n",
        "    x = layers.Conv2D(512, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Progressively upscale\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 16x16\n",
        "    x = layers.Conv2D(256, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 32x32\n",
        "    x = layers.Conv2D(128, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 64x64\n",
        "    x = layers.Conv2D(64, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 128x128\n",
        "    x = layers.Conv2D(32, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 256x256\n",
        "    x = layers.Conv2D(16, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 512x512\n",
        "    x = layers.Conv2D(8, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.UpSampling2D()(x)  # Upsampling to 1024x1024\n",
        "    x = layers.Conv2D(4, kernel_size=3, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Final output layer\n",
        "    img = layers.Conv2D(3, kernel_size=3, padding='same', activation='tanh')(x)\n",
        "\n",
        "    # Create and return the model\n",
        "    model = tf.keras.Model(inputs=style_input, outputs=img)\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator (Critic for WGAN)\n",
        "def build_discriminator():\n",
        "    input_img = layers.Input(shape=img_shape)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=2, padding='same')(input_img)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1)(x)  # No sigmoid activation for the critic\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_img, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Instantiate the models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005))\n",
        "\n",
        "# Build the WGAN model (combine generator and critic)\n",
        "discriminator.trainable = False  # During GAN training, freeze the discriminator\n",
        "\n",
        "z = layers.Input(shape=(latent_dim,))\n",
        "gen_img = generator(z)\n",
        "validity = discriminator(gen_img)\n",
        "\n",
        "wgan = tf.keras.Model(inputs=z, outputs=validity)\n",
        "\n",
        "# Compile the WGAN model (generator)\n",
        "wgan.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005))\n",
        "\n",
        "# Function to train the WGAN with Wasserstein Loss\n",
        "def train_wgan(epochs, batch_size, sample_interval=50):\n",
        "    # Load dataset (e.g., CIFAR-10, here we use random data for illustration)\n",
        "    (X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "    X_train = X_train / 127.5 - 1.0  # Rescale to [-1, 1]\n",
        "    X_train = np.expand_dims(X_train, axis=3)  # Add channel dimension\n",
        "\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train the Critic (Discriminator)\n",
        "\n",
        "        # Train the critic 5 times per generator update\n",
        "        for _ in range(5):  # Number of critic updates\n",
        "            # Select a random batch of real images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_images = X_train[idx]\n",
        "\n",
        "            # Generate fake images\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            fake_images = generator.predict(noise)\n",
        "\n",
        "            # Train the critic on real and fake images (Wasserstein loss)\n",
        "            c_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "            c_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "            c_loss = 0.5 * np.add(c_loss_real, c_loss_fake)\n",
        "\n",
        "            # Clip the critic's weights to enforce the Lipschitz constraint\n",
        "            for layer in discriminator.layers:\n",
        "                if hasattr(layer, 'kernel'):\n",
        "                    layer.kernel.assign(tf.clip_by_value(layer.kernel, -0.01, 0.01))\n",
        "\n",
        "        # Train the Generator (via the WGAN model)\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        g_loss = wgan.train_on_batch(noise, real_labels)\n",
        "\n",
        "        # Print the progress\n",
        "        if epoch % sample_interval == 0:\n",
        "            print(f\"{epoch} [C loss: {c_loss}] [G loss: {g_loss}]\")\n",
        "            sample_images(epoch)\n",
        "\n",
        "# Function to save generated images\n",
        "# Function to train the WGAN with Wasserstein Loss\n",
        "def train_wgan(epochs, batch_size, sample_interval=50):\n",
        "    # Load dataset (e.g., CIFAR-10, here we use random data for illustration)\n",
        "    (X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "    # Rescale and resize the images to the expected shape\n",
        "    # array_to_img converts the numpy array back to a PIL image, before using resize\n",
        "    # img_to_array converts the PIL image to a numpy array\n",
        "    from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "    X_train_resized = np.array([img_to_array(array_to_img(img).resize((1024, 1024))) for img in X_train])\n",
        "\n",
        "    X_train_resized = X_train_resized / 127.5 - 1.0  # Rescale to [-1, 1]\n",
        "\n",
        "\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train the Critic (Discriminator)\n",
        "\n",
        "        # Train the critic 5 times per generator update\n",
        "        for _ in range(5):  # Number of critic updates\n",
        "            # Select a random batch of real images\n",
        "            idx = np.random.randint(0, X_train_resized.shape[0], batch_size)\n",
        "            real_images = X_train_resized[idx]\n",
        "\n",
        "            # Generate fake images\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            fake_images = generator.predict(noise)\n",
        "\n",
        "            # Train the critic on real and fake images (Wasserstein loss)\n",
        "            c_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "            c_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "            c_loss = 0.5 * np.add(c_loss_real, c_loss_fake)\n",
        "\n",
        "            # Clip the critic's weights to enforce the Lipschitz constraint\n",
        "            for layer in discriminator.layers:\n",
        "                if hasattr(layer, 'kernel'):\n",
        "                    layer.kernel.assign(tf.clip_by_value(layer.kernel, -0.01, 0.01))\n",
        "\n",
        "        # Train the Generator (via the WGAN model)\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        g_loss = wgan.train_on_batch(noise, real_labels)\n",
        "\n",
        "        # Print the progress\n",
        "        if epoch % sample_interval == 0:\n",
        "            print(f\"{epoch} [C loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IahLU-2goHb1",
        "outputId": "c6f603f8-8878-4dee-cead-988f94732702"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.8.Write a function to modify the discriminator to include a dropout layer with a rate of 0.4 and print the\n",
        "configurations?"
      ],
      "metadata": {
        "id": "0bC_gBJlqL7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator_with_dropout():\n",
        "    input_img = layers.Input(shape=(32, 32, 3))  # Input shape for CIFAR-10 (32x32x3)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=2, padding='same')(input_img)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Add Dropout layer after the last convolutional block\n",
        "    x = layers.Dropout(0.4)(x)  # Dropout with rate 0.4\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1)(x)  # Output layer with no activation for WGAN\n",
        "\n",
        "    # Build the model\n",
        "    model = tf.keras.Model(inputs=input_img, outputs=x)\n",
        "\n",
        "    # Print the model configuration\n",
        "    model.summary()\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "T8F-NVHqqQqo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.9.Write a function to modify the discriminator to include a dropout layer with a rate of 0.4 and print the\n",
        "configurations?"
      ],
      "metadata": {
        "id": "Q2tgCLBmqTkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_discriminator_with_dropout():\n",
        "    input_img = layers.Input(shape=(1024, 1024, 3))  # Input shape for high-resolution images (e.g., 1024x1024x3)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=2, padding='same')(input_img)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Add Dropout layer with a rate of 0.4\n",
        "    x = layers.Dropout(0.4)(x)  # Dropout with a 40% drop rate\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1)(x)  # Output layer, no activation function for WGAN\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=input_img, outputs=x)\n",
        "\n",
        "    # Print the model configuration\n",
        "    model.summary()  # Prints the layer details, output shapes, and parameters\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "3VA3a1ghqaBH"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}